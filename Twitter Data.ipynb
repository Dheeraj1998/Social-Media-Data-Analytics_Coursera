{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from urllib.parse import urlparse \n",
    "import urllib\n",
    "import csv\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_fix(s, charset='utf-8'):\n",
    "    if isinstance(s, unicode):\n",
    "        s = s.encode(charset, 'ignore')\n",
    "    scheme, netloc, path, qs, anchor = urlparse.urlsplit(s)\n",
    "    path = urllib.quote(path, '/%')\n",
    "    qs = urllib.quote_plus(qs, ':&=')\n",
    "    return urlparse.urlunsplit((scheme, netloc, path, qs, anchor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tw_parser(query, location, language, tweet_type, tweet_count):\n",
    "    global qw, ge, l, t, c, d\n",
    "\n",
    "# USE EXAMPLES:\n",
    "# =-=-=-=-=-=-=\n",
    "# % twsearch <search term>            --- searches term\n",
    "# % twsearch <search term> -g sf      --- searches term in SF geographic box <DEFAULT = none>\n",
    "# % twsearch <search term> -l en      --- searches term with lang=en (English) <DEFAULT = en>\n",
    "# % twsearch <search term> -t {m,r,p} --- searches term of type: mixed, recent, or popular <DEFAULT = recent>\n",
    "# % twsearch <search term> -c 12      --- searches term and returns 12 tweets (count=12) <DEFAULT = 1>\n",
    "# % twsearch <search term> -o {ca, tx, id, co, rtc)   --- searches term and sets output options <DEFAULT = ca, tx>\n",
    "\n",
    "# Parse the command\n",
    "    #parser = argparse.ArgumentParser(description='Twitter Search')\n",
    "    #parser.add_argument(action='store', dest='query', help='Search term string')\n",
    "    #parser.add_argument('-g', action='store', dest='loca', help='Location (lo, nyl, nym, nyu, dc, sf, nb')\n",
    "    #parser.add_argument('-l', action='store', dest='l', help='Language (en = English, fr = French, etc...)')\n",
    "    #parser.add_argument('-t', action='store', dest='t', help='Search type: mixed, recent, or popular')\n",
    "    #parser.add_argument('-c', action='store', dest='c', help='Tweet count (must be <50)')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    qw = query     # Actual query word(s)\n",
    "    ge = ''\n",
    "\n",
    "    # Location\n",
    "    loca = location\n",
    "    if (not(loca in ('lo', 'nyl', 'nym', 'nyu', 'dc', 'sf', 'nb')) and (loca)):\n",
    "        print (\"WARNING: Location must be one of these: lo, nyl, nym, nyu, dc, sf, nb\")\n",
    "        #exit()\n",
    "    if loca:\n",
    "        ge = locords[loca]\n",
    "\n",
    "    # Language\n",
    "    l = language\n",
    "    if (not l):\n",
    "        l = \"en\"\n",
    "    if (not(l in ('en','fr','es','po','ko', 'ar'))):\n",
    "        print (\"WARNING: Languages currently supported are: en (English), fr (French), es (Spanish), po (Portuguese), ko (Korean), ar (Arabic)\")\n",
    "        #exit()\n",
    "\n",
    "    # Tweet type\n",
    "    t = tweet_type\n",
    "    if (not t):\n",
    "        t = \"recent\"\n",
    "    if (not(t in ('mixed','recent','popular'))):\n",
    "        print (\"WARNING: Search type must be one of: (m)ixed, (r)ecent, or (p)opular\")\n",
    "        #exit()\n",
    "\n",
    "    # Tweet count\n",
    "    if tweet_count:\n",
    "        c = int(tweet_count)\n",
    "        if (c > cmax):\n",
    "            print (\"Resetting count to \",cmax,\" (maximum allowed)\")\n",
    "            c = cmax\n",
    "        if (not (c) or (c < 1)):\n",
    "            c = 1\n",
    "    if not(c):\n",
    "        c = 1\n",
    "\n",
    "    print (\"Query: %s, Location: %s, Language: %s, Search type: %s, Count: %s\" %(qw,ge,l,t,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tw_oauth(authfile):\n",
    "    with open(authfile, \"r\") as f:\n",
    "        ak = f.readlines()\n",
    "    f.close()\n",
    "    auth1 = tweepy.auth.OAuthHandler(ak[0].replace(\"\\n\",\"\"), ak[1].replace(\"\\n\",\"\"))\n",
    "    auth1.set_access_token(ak[2].replace(\"\\n\",\"\"), ak[3].replace(\"\\n\",\"\"))\n",
    "    return tweepy.API(auth1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweepy search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tw_search_json(query, cnt=5):\n",
    "    authfile = './auth.k'\n",
    "    api = tw_oauth(authfile)\n",
    "    results = {}\n",
    "    meta = {\n",
    "        'username': 'text',\n",
    "        'usersince': 'date',\n",
    "        'followers': 'numeric',\n",
    "        'friends': 'numeric',\n",
    "        'authorid': 'text',\n",
    "        'authorloc': 'geo',\n",
    "        'geoenable': 'boolean',\n",
    "        'source': 'text'\n",
    "    }\n",
    "    data = []\n",
    "    for tweet in tweepy.Cursor(api.search, q=query, count=cnt).items():\n",
    "        dTwt = {}\n",
    "        dTwt['username'] = tweet.author.name\n",
    "        dTwt['usersince'] = tweet.author.created_at      #author/user profile creation date\n",
    "        dTwt['followers'] = tweet.author.followers_count #number of author/user followers (inlink)\n",
    "        dTwt['friends']   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
    "        dTwt['authorid']  = tweet.author.id              #author/user ID#\n",
    "        dTwt['authorloc'] = tweet.author.location        #author/user location\n",
    "        dTwt['geoenable'] = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
    "        dTwt['source']    = tweet.source                 #platform source for tweet\n",
    "        data.append(dTwt)\n",
    "    results['meta'] = meta\n",
    "    results['data'] = data\n",
    "    return results\n",
    "\n",
    "def tw_search(api):\n",
    "    counter = 0\n",
    "    # Open/Create a file to append data\n",
    "    csvFile = open('result.csv','w')\n",
    "    #Use csv Writer\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow([\"created\", \"text\", \"retwc\", \"hashtag\", \"followers\", \"friends\"])\n",
    "\t\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                                q = qw,\n",
    "                                g = ge,\n",
    "                                lang = l,\n",
    "                                result_type = t,\n",
    "                                count = c).items():\n",
    "\n",
    "        #TWEET INFO\n",
    "        created = tweet.created_at   #tweet created\n",
    "        text    = tweet.text         #tweet text\n",
    "        tweet_id = tweet.id          #tweet ID# (not author ID#)\n",
    "        cords   = tweet.coordinates  #geographic co-ordinates\n",
    "        retwc   = tweet.retweet_count #re-tweet count\n",
    "        try:\n",
    "            hashtag = tweet.entities[u'hashtags'][0][u'text'] #hashtags used\n",
    "        except:\n",
    "            hashtag = \"None\"\n",
    "        try:\n",
    "            rawurl = tweet.entities[u'urls'][0][u'url'] #URLs used\n",
    "            urls = url_fix(rawurl)\n",
    "        except:\n",
    "            urls    = \"None\"\n",
    "        #AUTHOR INFO\n",
    "        username  = tweet.author.name            #author/user name\n",
    "        usersince = tweet.author.created_at      #author/user profile creation date\n",
    "        followers = tweet.author.followers_count #number of author/user followers (inlink)\n",
    "        friends   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
    "        authorid  = tweet.author.id              #author/user ID#\n",
    "        authorloc = tweet.author.location        #author/user location\n",
    "        #TECHNOLOGY INFO\n",
    "        geoenable = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
    "        source    = tweet.source                 #platform source for tweet\n",
    "\t\t# Dongho 03/28/16\n",
    "        csvWriter.writerow([created, str(text).encode(\"utf-8\"), retwc, hashtag, followers, friends])\n",
    "        counter = counter +1\n",
    "        if (counter == c):\n",
    "            break\n",
    "\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: game of thrones, Location: 0, 51.503, 20km, Language: en, Search type: recent, Count: 50\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    global api, cmax, locords\n",
    "\n",
    "    # Geo-coordinates of five metropolitan areas\n",
    "    # London, NYC (lower, middle, upper), Wash DC, San Francisco, New Brunswick (NJ)\n",
    "    locords =  {'lo': '0, 51.503, 20km',\n",
    "                'nyl': '-74, 40.73, 2mi',\n",
    "                'nym': '-74, 40.74, 2mi',\n",
    "                'nyu': '-73.96, 40.78, 2mi',\n",
    "                'dc': '-77.04, 38.91, 2mi',\n",
    "                'sf': '-122.45, 37.74, 5km',\n",
    "                'nb': '-74.45, 40.49, 2mi'}\n",
    "    # Maximum allowed tweet count (note: Twitter sets this to ~180 per 15 minutes)\n",
    "    cmax = 50\n",
    "    # OAuth key file\n",
    "    authfile = './auth.k'\n",
    "\n",
    "    tw_parser('game of thrones','lo','en','recent',50)\n",
    "    api = tw_oauth(authfile)\n",
    "    tw_search(api)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
